{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90463b0b-d2f6-4fa4-b184-320439abcb6a",
   "metadata": {},
   "source": [
    "# Sequential Decision Making Under Uncertainty\n",
    "\n",
    "In reinforcement learning, an agent generates its own training data by interacting with the world. Unlike supervised learning, where correct actions are provided, the agent must discover the consequences of its actions through trial and error. We introduce the fundamentals of reinforcement learning (RL) by focusing on the evaluative aspect of decision-making under uncertainty. We'll explore how agents learn from interactions with their environment, emphasizing key concepts like rewards, timesteps, and values. The core framework is the k-armed bandit problem, a simplified setting that captures essential RL ideas.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "996b4a99-dc70-44ff-a738-0493244b9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf3abff8-a144-4fc4-bb7a-b6d09bc6c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/dksifoua/Developer/learning/Reinforcement-Learning\n"
     ]
    }
   ],
   "source": [
    "if str(os.getcwd()).endswith(\"notebooks\"):\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed3db6-39e9-480c-993a-f38394eb0cf6",
   "metadata": {},
   "source": [
    "## 1. The K-Armed Bandit problem\n",
    "\n",
    "In a k-armed bandit problem, an **agent** is faced repeatedly with a choice among k different **actions**. After taking each action, it receives a numerical **reward** chosen from a **stationary probability distribution** that depends on the selected action. The objective is **to maximize the expected total reward over some time period**, for example, over 1000 action selections, or **time steps**.\n",
    "\n",
    "Each of the k actions has an expected or mean reward given that that action is selected; let us call this the **value** of that action. We denote the action selected on time step $t$ as $A_t$, and the corresponding reward as $R_t$. The value of an arbitrary action $a$, denoted $q_*(a)$, is the expected reward given that a is selected:\n",
    "\n",
    "$$ q_*(a) \\; \\dot{=} \\; \\mathbb{E}[R_t|A_t = a] $$\n",
    "\n",
    "In the k-armed bandit setting, we do not know the action values with certainty (the action values with certainty). We denote the estimated value of action $a$ at time step $t$ as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_*(a)$.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e80e05e-846c-411f-9134-aef7c3554a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBanditEnvironment:\n",
    "    \"\"\"K-armed bandit environment with stationary action means.\n",
    "\n",
    "    Each action ``a`` has a fixed (hidden) mean reward ``Q*(a)`` sampled once at\n",
    "    reset from ``Normal(initial_reward, 1)``. Calling :meth:`step` returns a noisy\n",
    "    reward drawn from ``Normal(Q*(a), reward_noise_std^2)``.\n",
    "\n",
    "    Args:\n",
    "        n_actions (int): Number of available actions (arms). Must be > 0.\n",
    "        initial_reward (float): Mean around which action means are initialized.\n",
    "        reward_noise_std (float, optional): Standard deviation of the observation\n",
    "            noise added on each :meth:`step`. Defaults to ``1.0``. Must be >= 0.\n",
    "        seed (int | None, optional): Random seed for reproducibility. Ignored if\n",
    "            ``rng`` is provided. Defaults to ``None``.\n",
    "        rng (np.random.Generator | None, optional): Custom NumPy random generator.\n",
    "            If ``None``, a new one is created (optionally seeded). Defaults to ``None``.\n",
    "\n",
    "    Attributes:\n",
    "        n_actions (int): Number of actions.\n",
    "        initial_reward (float): Initialization center for action means.\n",
    "        reward_noise_std (float): Observation noise standard deviation.\n",
    "        rng (np.random.Generator): Random generator used by the environment.\n",
    "        rewards (np.ndarray): Per-action means ``Q*(a)`` sampled at last reset.\n",
    "            Shape: ``(n_actions,)``.\n",
    "        best_action (int): Index of the action with the highest mean at last reset.\n",
    "\n",
    "    Examples:\n",
    "        >>> env = KArmedBanditEnvironment(n_actions=10, initial_reward=0.0, seed=123)\n",
    "        >>> env.reset() # resample action means\n",
    "        >>> r = env.step(0) # take action 0, observe reward\n",
    "        >>> env.best_action in range(env.n_actions)\n",
    "        True\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = (\n",
    "        \"n_actions\",\n",
    "        \"initial_reward\",\n",
    "        \"reward_noise_std\",\n",
    "        \"rng\",\n",
    "        \"rewards\",\n",
    "        \"best_action\",\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_actions: int,\n",
    "        initial_reward: float,\n",
    "        reward_noise_std: float = 1.0,\n",
    "        seed: Optional[int] = None,\n",
    "        rng: Optional[np.random.Generator] = None,\n",
    "    ) -> None:\n",
    "        if n_actions <= 0:\n",
    "            raise ValueError(\"n_actions must be a positive integer.\")\n",
    "        if reward_noise_std < 0:\n",
    "            raise ValueError(\"reward_noise_std must be non-negative.\")\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.initial_reward = initial_reward\n",
    "        self.reward_noise_std = reward_noise_std\n",
    "        self.rng = rng if rng is not None else np.random.default_rng(seed)\n",
    "\n",
    "        self.rewards = np.empty(self.n_actions, dtype=float)\n",
    "        self.best_action: Optional[int] = None\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resample per-action mean rewards.\n",
    "\n",
    "        The per-action means ``Q*(a)`` are drawn from ``Normal(initial_reward, 1)``.\n",
    "        \"\"\"\n",
    "        self.rewards = self.rng.standard_normal(self.n_actions) + self.initial_reward\n",
    "        self.best_action = np.argmax(self.rewards)\n",
    "\n",
    "    def step(self, action: int) -> float:\n",
    "        \"\"\"Take an action and observe a noisy reward.\n",
    "\n",
    "        Args:\n",
    "            action (int): Index of the chosen action in ``[0, n_actions)``.\n",
    "\n",
    "        Returns:\n",
    "            float: Observed reward drawn from\n",
    "                ``Normal(Q*(action), reward_noise_std^2)``.\n",
    "\n",
    "        Raises:\n",
    "            IndexError: If ``action`` is out of bounds.\n",
    "        \"\"\"\n",
    "        if self.best_action is None:\n",
    "            raise RuntimeError(\"Environment not ready. Call `reset()` before the first `step()`.\")\n",
    "        \n",
    "        if not (0 <= action < self.n_actions):\n",
    "            raise IndexError(f\"action must be in [0, {self.n_actions - 1}], got {action}\")\n",
    "            \n",
    "        noise = 0.0 if self.reward_noise_std == 0.0 else self.rng.normal(0.0, self.reward_noise_std)\n",
    "        return self.rewards[action] + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d874b4-a701-4840-abf5-6b55a4035bad",
   "metadata": {},
   "source": [
    "## 2. Action-Value Methods\n",
    "\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21eb46-8df4-42a7-9a51-6279913ee774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
