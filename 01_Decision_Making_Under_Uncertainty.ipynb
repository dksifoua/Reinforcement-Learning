{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01 - Decision Making Under Uncertainty.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4h8y50TjQea",
        "colab_type": "text"
      },
      "source": [
        "## Sequential decision making with evaluative feedback\n",
        "\n",
        "In RL, the agent generates its own training data by interacting with the world. The agent must learn the consequences of its own actions through trials and errors rather than being told the correct actions.\n",
        "\n",
        "**The K-armed bandit problem**\n",
        "\n",
        "In a k-armed bandit problem, we have an **agent** who chooses between $k$ **actions** and receives a **reward** based on the action it chooses.\n",
        "\n",
        "**Action-Values**\n",
        "\n",
        "For the agent decides which action is best, we must define a velue of taking each action called **action-values function**.\n",
        "\n",
        "- The **value** is the **expected reward** the agent receives when taking an action.\n",
        "\n",
        "$$q_*(a) \\; \\dot{=} \\; \\mathbb{E}[R_t|A_t=a] \\;\\; \\forall a \\in \\{1, ..., k\\} \\\\\n",
        "= \\sum_r p(r|a) \\; r$$\n",
        "\n",
        "- The goal of the agent is to **maximize** the **expected reward** by selecting the action that have the highest value.\n",
        "\n",
        "$$argmax_a \\; q_*(a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9rkYRkXvnul",
        "colab_type": "text"
      },
      "source": [
        "## Learning Action-Values\n",
        "\n",
        "The objective is to **estimate** $Q(a) \\rightarrow q_*(a)$.\n",
        "\n",
        "**Sample-Average Method**\n",
        "\n",
        "One way to estimate $q_*(a)$ is to compute a sample average:\n",
        "\n",
        "$$Q_t(a) \\; \\dot{=} \\; \\frac{\\sum_{i=1}^{t-1}R_i \\; I[A_i = a]}{\\sum_{i=1}^{t-1}\\; I[A_i = a]}$$\n",
        "\n",
        "If the denominator is $0$, we defined $Q(a)$ at some default value such as $0$.\n",
        "\n",
        "As $\\sum_{i=1}^{t-1}\\; I[A_i = a] \\rightarrow +\\infty$, $\\;Q_t(a) \\rightarrow q_*(a)$ (*Law of large numbers*).\n",
        "\n",
        "The sample-average method is not necessarily the best one for selecting action-values.\n",
        "\n",
        "Calculating $Q_t(a)$ by using the formula above implies memory and computational requirements which increase as $t$ increases. So, we  use an **incremental implementation**\n",
        "\n",
        "Let's $Q_n = \\frac{R_1 + R_2 + ... + R_{n-1}}{n - 1}$\n",
        "\n",
        "$$\\Rightarrow Q_{n + 1} = \\frac{1}{n} \\sum_{i=1}^n R_i = \\frac{1}{n} [R_n + \\sum_{i=1}^{n-1} R_i] = \\frac{1}{n} [R_n + (n - 1) \\frac{1}{n-1} \\sum_{i=1}^{n-1} R_i] = \\frac{1}{n} [R_n + (n-1)Q_n] = \\frac{1}{n} (R_n + nQ_n  - Q_n) = Q_n + \\frac{1}{n} (R_n - Q_n)$$\n",
        "\n",
        "We obtain the update rule: **New Estimate $\\leftarrow$ Old Estimate + Step Size [Target - Old Estimate]**\n",
        "\n",
        "Step Size = $\\alpha = \\alpha_t(a) = \\frac{1}{n} \\in [0, 1)$\n",
        "\n",
        "**Tracking non-stationary problem - Exponential recency-weighted average**\n",
        "\n",
        "When the probability of reward changes over time (in most RL problems), it makes sense to give more weight to recent rewards than to long-past rewards.\n",
        "\n",
        "$$Q_{n+1} = Q_n + \\alpha (R_n - Q_n) = \\alpha R_n + (1 - \\alpha) Q_n = \\alpha R_n + (1 - \\alpha) [\\alpha R_{n-1} + (1 - \\alpha) Q_{n - 1}] = \\alpha R_n + (1 - \\alpha) \\alpha R_{n - 1} + (1 - \\alpha)^2 Q_{n - 1} = \\alpha R_n + (1 - \\alpha) \\alpha R_{n - 1} + (1 - \\alpha)^2 [\\alpha R_{n-2} + (1 - \\alpha) Q_{n - 2}] = \\alpha R_n + (1 - \\alpha) \\alpha R_{n - 1} + (1 - \\alpha)^2 \\alpha R_{n - 2} + (1 - \\alpha)^3 Q_{n - 2} = \\;...$$\n",
        "$$\\Rightarrow Q_{n+1} = (1 - \\alpha)^n Q_1 + \\sum_{i=1}^n \\alpha (1 - \\alpha)^{n-i}R_i$$\n",
        "\n",
        "The convergence is guarantedd with $\\alpha_n(a) = \\frac{1}{n}$.\n",
        "\n",
        "Sometimes, it is convenient to change $\\alpha_n(a)$ from step to step. But of course convergence is guaranteed for all choices of the sequence $\\{\\alpha_n(a)\\}$.\n",
        "\n",
        "A well-known result in stochastic approximation theory gives us the conditions required to ensure convergence with probability 1:\n",
        "\n",
        "1. $\\sum_{n=1}^\\infty \\alpha_n(a) = \\infty$ guaranteed that the steps are large enough to eventually overcome any init conditions or random fluctuations.\n",
        "2. $\\sum_{n=1}^\\infty \\alpha_n(a) < \\infty$ guaranteed that eventually the steps become small enough to ensure convergence.\n",
        "\n",
        "These conditions are **true** with $\\alpha_n(a) = \\frac{1}{n}$ but **false** for the case of constant stepsize param $\\alpha$ (which is desirable in non-stationary environments).\n",
        "\n",
        "**Action selection**\n",
        "\n",
        "- **Greedy action selection** (low reward variance): the agent always exploits its current knowledge to maximize the immediate reward.\n",
        "\n",
        "$$A_t = argmax_a \\; Q_t(a)$$\n",
        "\n",
        "- **$\\varepsilon$-greedy action selection** (high reward variance): every once in a while, the agent an action with a small probability $\\varepsilon$. Every action will be sampled an infinite number of time as the number of steps increase. Thus, $Q_t(a) \\rightarrow q_*(a) \\Rightarrow P(a_t^*) \\rightarrow 1 - \\varepsilon$.\n",
        "\n",
        "If the reward function is non-stationary (doesn't change over time), the exploration is needed i.e $\\varepsilon$-greedy action selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nINCphLdvpPQ",
        "colab_type": "text"
      },
      "source": [
        "## Exploration vs Exploitation tradeoff\n",
        "\n",
        "The tradeoff is simply the way the agent decides when its takes the best action (according to its current knowledge) - exploitation or try something else (random) exploration. If the agent never chooses a particular action, it won't know its value.\n",
        "\n",
        "- **Exploration**: *improve* knowledge for *long-term* benefit.\n",
        "\n",
        "- **Exploitation**: *exploit* knowledge for *short-term* benefit.\n",
        "\n",
        "When we explore, we get more accurate estimate of our values, when we exploit, we might get more reward. We cannot however choose to do both simultaneously.\n",
        "\n",
        "A simple method to choose between exploration and exploitation is to use **$\\varepsilon$-greedy action selection**. Other methods are **optimistic initial values** and **Upper-Confidence Bound (UCB) action selection**.\n",
        "\n",
        "**Optimistic initial values**\n",
        "\n",
        "All methods discussed so far depend on init value $Q_1(a)$. These methods are **biased** since we use the statistic mean. The bias disappears with sampled average and is constant with fixed value of $\\alpha$.\n",
        "\n",
        "In practice, it's not usually a problem and can sometimes be very helpfull.\n",
        "\n",
        "**Limitations of optimistic initial values**\n",
        "\n",
        "- Only drives early exploration.\n",
        "- Not well-suited for non-stationary problems.\n",
        "- Sometimes, it's difficult to know what the optimistic initial values should be. Then, we have to tune them.\n",
        "\n",
        "**Upper-Confidence Bound (UCB) action selection**\n",
        "\n",
        "UCB action selection uses **uncertainty** in the value estimates for balancing exploration and exploitation.\n",
        "\n",
        "In other words, it would be better to select among non-greedy actions according to their potential for being actually optimal taking into account:\n",
        "\n",
        "- How close the estimates are being max.\n",
        "- The uncertainties in those estimates.\n",
        "\n",
        "One effetive way to do that is:\n",
        "\n",
        "$$A_t = argmax_a [Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}}]$$\n",
        "\n",
        "Where $N_t(a)$ is the number of time the action $a$ is selected at time step $t$ and $c$ control the exploration.\n",
        "\n",
        "UCB action selection always performs well than $\\varepsilon$-greedy action selection but it's **more difficult** to extend to more general RL problems:\n",
        "\n",
        "- Deal with non-stationary problems.\n",
        "- Deal with large state spaces particularly using function approximation.\n",
        "\n",
        "In these more advanced settings, UCB action selection is usually not practical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLECYrO2EFjp",
        "colab_type": "text"
      },
      "source": [
        "## Implementation of Bandit Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtTyxdeAigzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from typing import Any, Dict, List, Union"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ig00APg9jl9",
        "colab_type": "text"
      },
      "source": [
        "### Argmax function\n",
        "\n",
        "First, we are going to implement an `argmax` function in order to get the best action value. Numpy's argmax function returns the first instance of the highest value. We do not want that to happen as it biases the agent to choose a specific action in the case of ties. Instead we want to break ties between the highest values randomly. So we are going to implement our own argmax function. You may want to look at np.random.choice to randomly select from a list of values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3gdGV2E4MDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def argmax(q_values: np.array):\n",
        "    \"\"\"\n",
        "    Return the argmax of q_values. If there's ties, we randomly select one of \n",
        "    them.\n",
        "\n",
        "    Args:\n",
        "        q_values: np.array\n",
        "            The action values\n",
        "    \n",
        "    Returns: int\n",
        "        The index representing the action to be taken\n",
        "    \"\"\"\n",
        "    max_q = float('-inf')\n",
        "    ties = []\n",
        "    for index, q_value in enumerate(q_values):\n",
        "        if q_value >= max_q:\n",
        "            if q_value == max_q:\n",
        "                ties.append(index)\n",
        "            else:\n",
        "                ties = [index]\n",
        "            max_q = q_value\n",
        "    return ties[np.random.choice(len(ties))]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zJbpUKp7aBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_argmax():\n",
        "    q_values = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
        "    assert argmax(q_values) == 8, f'argmax={argmax(q_values)} != 8'\n",
        "\n",
        "    q_values = np.array([1, 0, 0, 1])\n",
        "    total = 0\n",
        "    for i in range(100):\n",
        "        total += argmax(q_values)\n",
        "\n",
        "    assert total > 0, 'Total is less or equal than 0'\n",
        "    assert total != 300, f'Total should be different of 300 ({total})'\n",
        "\n",
        "test_argmax()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nNXaAmhSkDc",
        "colab_type": "text"
      },
      "source": [
        "### Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyOjeCojSmdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Environment:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coS0fZMO4OFo",
        "colab_type": "text"
      },
      "source": [
        "### Agents\n",
        "\n",
        "***Greedy agent***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLLdFCaD77YW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GreedyAgent:\n",
        "    \"\"\"\n",
        "    Agent that implements Greedy action selection.\n",
        "\n",
        "    Args:\n",
        "        q_values: np.ndarray\n",
        "        arm_count: np.ndarray\n",
        "        last_action: int\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.q_values = None\n",
        "        self.arm_count = None\n",
        "        self.last_action = None\n",
        "\n",
        "    def initi_configs(self, n_actions: int,\n",
        "                      initial_values: Union[np.ndarray, float]):\n",
        "        self.q_values = np.ones((n_actions,)) * initial_values\n",
        "        self.arm_count = np.zeros((n_actions,))\n",
        "        self.last_action = 0\n",
        "\n",
        "    def step(self, reward: float):\n",
        "        action = argmax(self.q_values)\n",
        "        self.arm_count[self.last_action] += 1\n",
        "        self.q_values[self.last_action] += (\n",
        "            1 / self.arm_count[self.last_action]\n",
        "        ) * (reward - self.q_values[self.last_action])\n",
        "        self.last_action = action\n",
        "        return action"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG8j7NleGe0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_greedy_agent():\n",
        "    agent = GreedyAgent()\n",
        "    agent.q_values = np.array([0, 0, 1.0, 0, 0])\n",
        "    agent.arm_count = np.array([0, 1, 0, 0, 0])\n",
        "    agent.last_action = 1\n",
        "    action = agent.step(reward=1)\n",
        "    assert action == 2\n",
        "    assert (agent.q_values == np.array([0, 0.5, 1.0, 0, 0])).all()\n",
        "\n",
        "test_greedy_agent()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BWHmQGGIv2V",
        "colab_type": "text"
      },
      "source": [
        "***Epsilon Greedy Agent***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPJ6swR8HOup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EpsilonGreedyAgent:\n",
        "    \"\"\"\n",
        "    Agent that implements Epsilon-Greedy action selection.\n",
        "\n",
        "    Args:\n",
        "        q_values: np.ndarray\n",
        "        arm_count: np.ndarray\n",
        "        epsilon: float\n",
        "        last_action: int\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.q_values = None\n",
        "        self.arm_count = None\n",
        "        self.epsilon = None\n",
        "        self.last_action = None\n",
        "\n",
        "    def initi_configs(self, n_actions: int,\n",
        "                      initial_values: Union[np.ndarray, float], epsilon: float):\n",
        "        self.q_values = np.ones((n_actions,)) * initial_values\n",
        "        self.arm_count = np.zeros((n_actions,))\n",
        "        self.epsilon = epsilon\n",
        "        self.last_action = 0\n",
        "\n",
        "    def step(self, reward: float):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.q_value.size)\n",
        "        else:\n",
        "            action = argmax(self.q_values)\n",
        "        self.arm_count[self.last_action] += 1\n",
        "        self.q_values[self.last_action] += (\n",
        "            1 / self.arm_count[self.last_action]\n",
        "        ) * (reward - self.q_values[self.last_action])\n",
        "        self.last_action = action\n",
        "        return action"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qsm0EQrLMgF",
        "colab_type": "text"
      },
      "source": [
        "***Epsilon-Greedy Agent with Constant Step-size***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtZKFTh_Ku5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EpsilonGreedyAgentWithConstantStepSize:\n",
        "    \"\"\"\n",
        "    Agent that implements Epsilon-Greedy action selection with constant \n",
        "    step-size.\n",
        "\n",
        "    Args:\n",
        "        q_values: np.ndarray\n",
        "        step_size: float\n",
        "        epsilon: float\n",
        "        last_action: int\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.q_values = None\n",
        "        self.step_size = None\n",
        "        self.epsilon = None\n",
        "        self.last_action = None\n",
        "\n",
        "    def initi_configs(self, n_actions: int, step_size: float, epsilon: float,\n",
        "                      initial_values: Union[np.ndarray, float]):\n",
        "        self.q_values = np.ones((n_actions,)) * initial_values\n",
        "        self.step_size = step_size\n",
        "        self.epsilon = epsilon\n",
        "        self.last_action = 0\n",
        "\n",
        "    def step(self, reward: float):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.q_value.size)\n",
        "        else:\n",
        "            action = argmax(self.q_values)\n",
        "        self.q_values[self.last_action] += self.step_size \\\n",
        "            * (reward - self.q_values[self.last_action])\n",
        "        self.last_action = action\n",
        "        return action"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AujqLFBMqno",
        "colab_type": "text"
      },
      "source": [
        "***Softmax Agent***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH5-JayPMaA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxAgent:\n",
        "    \"\"\"\n",
        "    Agent that implements Softmax action selection.\n",
        "\n",
        "    Args:\n",
        "        q_values: np.ndarray\n",
        "        arm_count: np.ndarray\n",
        "        last_action: int\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.q_values = None\n",
        "        self.arm_count = None\n",
        "        self.last_action = None\n",
        "\n",
        "    def initi_configs(self, n_actions: int,\n",
        "                      initial_values: Union[np.ndarray, float]):\n",
        "        self.q_values = np.ones((n_actions,)) * initial_values\n",
        "        self.arm_count = np.zeros((n_actions,))\n",
        "        self.last_action = 0\n",
        "\n",
        "    def step(self, reward: float):\n",
        "        probs = np.exp(self.q_values) / np.exp(self.q_values).sum()\n",
        "        action = np.random.choice(self.q_values, p=probs)\n",
        "        self.arm_count[self.last_action] += 1\n",
        "        self.q_values[self.last_action] += (\n",
        "            1 / self.arm_count[self.last_action]\n",
        "        ) * (reward - self.q_values[self.last_action])\n",
        "        self.last_action = action\n",
        "        return action"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJJ4f8_WPZVK",
        "colab_type": "text"
      },
      "source": [
        "***Softmax Agent with Constant Step-Size***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J1Inh9bPLKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxAgentWithConstantStepSize:\n",
        "    \"\"\"\n",
        "    Agent that implements Sotfmax action selection with constant step-size.\n",
        "\n",
        "    Args:\n",
        "        q_values: np.ndarray\n",
        "        step_size: float\n",
        "        last_action: int\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.q_values = None\n",
        "        self.step_size = None\n",
        "        self.last_action = None\n",
        "\n",
        "    def initi_configs(self, n_actions: int, step_size: float,\n",
        "                      initial_values: Union[np.ndarray, float]):\n",
        "        self.q_values = np.ones((n_actions,)) * initial_values\n",
        "        self.step_size = step_size\n",
        "        self.last_action = 0\n",
        "\n",
        "    def step(self, reward: float):\n",
        "        probs = np.exp(self.q_values) / np.exp(self.q_values).sum()\n",
        "        action = np.random.choice(self.q_values, p=probs)\n",
        "        self.q_values[self.last_action] += self.step_size \\\n",
        "            * (reward - self.q_values[self.last_action])\n",
        "        self.last_action = action\n",
        "        return action"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v1JVUt7TDNk",
        "colab_type": "text"
      },
      "source": [
        "### Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK2p1GLPTE3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Experiment:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}